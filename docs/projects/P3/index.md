# Project 3: Project Specification

## Learning Goals

- Negotiate a project plan with your partner organization and identify deliverables
- Translating requirements into actionable user stories
- Determine the project timeline
- Consider the human subjects that will be impacted by your project
- Learn to visualize your ideas in a wireframe


## Project Context

Your team has been formed and has normed. It is time to perform. You have several tasks ahead of you.

1. Meet with your partner organization to discuss your project ideas and negotiate what you will do for them. 
1. Work with the partner organization to identify an initial set of project requirements.
1. Turn those requirements into user stories and enter them into your project management system.
1. Divvy up the user stories and project requirements and assign responsibilities to each team member.
1. Create a [Gantt chart](https://en.wikipedia.org/wiki/Gantt_chart) for your user stories and project deliverables. This will be your project timeline.
1. Begin your design with a UI mockup, a wireframe, or a storyboard to illustrate the concept you want to make.

## Deliverables

### Partner Organization Meeting

You must set up a meeting with your partner organization ASAP. This can be in person or via video conferencing. Record the meeting if possible, so you have a reference to refer to in case you forget what you agreed to. Assign one member of your team to take notes in a Google Doc.

In this meeting, you should talk through what your partner organization would like you to do. You may offer alternative ideas, especially those you presented in your Project Pitches. Discuss all your options, while always considering your relatively short 7-8 week project timeline to ensure you can get everything done on time. Come to an agreement about what you want to do for them. The project must be either an accessibility evaluation project or an application solution.

An **accessibility evaluation** and improvement project will evaluate a pre-existing application or web site from the partner organization for accessibility by their target clientelle. You will engage with several members of their clientelle to do an in-person evaluation. Then you will design improvements and conduct a usability evaluation on the results. Your deliverables are an updated application design or web site as well as a project report describing the results of the initial accessibility study, the re-design process, and the evaluation of the redesign.

An **application solution** design and evaluation project will create a new application solution for an accessibility problem from the partner organization. You will engage in a co-design process with classmates and clients of the partner organization, develop the new application, web site, or tool, and then evaluate your application with additional clients of the partner organization. Your deliverables are the new application solution and a project report describing your co-design process, design mockups, final design, application documentation, and a writeup of the evaluation of the application. 

Continue negotiating if the project idea does not fit into one of these two project types.

Turn in your notes as part of the deliverable for this assignment.

### Requirements

Once you have agreed to a project concept, meet with your team members to identify the project requirements. This may involve communicating with the partner organization again to get answers to any questions or ambiguities you need to resolve.

Turn in a list of these requirements as part of the deliverable for this assignment. 

### User Stories

Discuss potential functional requirements of this project. Consider what possible use cases may be for this system and what features it should have to fulfill those needs. 

Then, document these functional requirements in the form of user stories. Use whatever user story format you have learned in class or used in prior group projects. 

**You should come up with at least two user stories per student in your group.**

!!! note "Formulating User Stories"
    Consider what are the different types of users that will be interacting with your system (your stakeholders) and what features they would want to have. You may want to reference features in existing systems that may be desirable, or conduct interviews with your peers who could be potential users of this system.

As a team, you should then come up with a prioritization ranking for each user story. The prioritization should be based on two factors 

- **Impact**: how essential is this user story to the overall functionality of the application to your stakeholders, how beneficial it would be to your stakeholders, and
- **Effort**: how much time/effort is required to implement this user story

Once you have your list of user stories, add them to your project management system. In the body of each of these user stories, provide a brief but concrete justification of its prioritization ranking that your team decided on. You should order all the user stories in this column from highest to lowest priority.

As part of your project deliverable, we will read through the user stories on your project management system.

### Mini-IRB 

We want to give you the flavor of what it is like to do academic user research. As you develop your project idea and project plan, we ask you to fill out an Institutional Review Board protocol documentation form. This will be a (very) simplified version of the form that CMU requires all researchers to fill out when doing user studies. It will prompt you to be very specific about the kinds of users you will engage with your project process and require that you plan, in detail, what you intend to do with them, way before you ever get one of them in a room. 

Fill out this form and include it as part of your project deliverable.

### Gantt Chart

Now that you have your prioritized list of user stories, consider the *technical requirements* of the various user stories and collectively decide on which one(s) you will be focusing on over the next two milestones. In this project (as like most projects), your team is aiming to maximize the amount of value you are delivering to your stakeholders given your constraints.

Your selected user stories should have relatively high priority based on your team's ranking, and you should actively take factors into consideration that may impact your development.

!!! note "Selecting Appropriate User Stories"
    Given the amount of variations in each team's user stories, it's hard to give concrete guideline on the number of user stories that a team needs to tackle. Teams could tackle 1 user story that requires major effort, or a few user stories that each requires lesser effort. 
    
    In general, we are expecting that user stories be selected given:

    - 2 milestones of about 2 weeks each
    - number of team members on your team
    - assumption of 9 hours/week available per individual

    The course staff **is happy to discuss this with your team during office hours** and we highly recommend you do so if your team is unsure. We will also be providing you with feedback during your first milestone.


Most project management systems have the ability to turn your prioritized user stories into a Gantt Chart. Take a snapshot of this Gantt chart and turn it in as part of your project deliverable. You will want to keep this chart up to date as whenever you meet with the course instructors about the project, we will ask to see your current Gantt Chart. 

### User Story Assignment

Assign every user story expected to be done in the first milestone to a member of your project team. Be sure this assignment is visible in the project management system. Over time, you may reassign user stories to balance your team's workload.

### UI Mockup

Now that your project concept has been nailed down and you've agreed on how to split up the work, it is time to develop your first UI design that can help guide your team towards producing the right thing. In a design tool like Canva or Figma, create a wireframe mockup of your project. This could be a storyboard or a sequence of design graphics.  

For your project deliverable, include a snapshot of your UI mockups.


## Deliverables and Deadlines

There is one **group deliverable** for this project. This part is worth 168 points, or 20% of your project grade. This part is due October 24, 2024 11:59pm. 

To receive full credit for the project specification, we expect that all sections listed above are addressed in a PDF document submitted to Gradescope. 



**Checkpoint Deliverables** – 35 points – due Friday, March 15th, 11:59pm

- [Deployed Application (25 pts)](#deployed-application-25-pts)
- [Tools Checkpoint (10 pts)](#tools-checkpoint-10-pts)

**Final Deliverables** – 65 points – due Thursday, March 21st, 11:59pm 

- [Tool Analysis Design Doc (50 pts)](#tool-analysis-design-doc-60-pts)
- [Tool Integration (15 pts)](#tool-integration-15-pts)

**Extra Credit (Individual)** - 6 points - due Thursday, March 21st, 11:59pm

- [Feature Review (6 pts)](#feature-review-6-pts)

!!! info "Work Distribution"
    There are two main focuses in this project: deployment and static/dynamic analysis. For the purposes of equitable distribution of labor, we recommend that you nominate one of your members to act as the Site Reliability Engineer(SRE) for this assignment who will be primarily responsible for deployment, and have all other teammates focus on tool research and integration. 


## Checkpoint Deliverables

### Deployed Application (25 pts)

Your team will be using Google Cloud Platform for the deployment of the NodeBB application. Further instructions on how to deploy can be found [here](/projects/P3/deployment).

Once you have successfully deployed your website, make sure to test within your team to ensure that your added feature(s) from Project 2 are properly integrated. 

By the checkpoint deadline you should

- Submit a link to the deployed site onto Gradescope
- Add your deployed site to this [public spreadsheet](https://docs.google.com/spreadsheets/d/1OjXgULfHYlSKW_4aPmBGwMOAJhz_Zww5iFe1rvwSlpQ/edit?usp=sharing), alongside your team name & UserGuide.md that your team submitted for Project 2. This will be used in [Feature Review](#feature-review-6-pts) for extra credit.

### Tools Checkpoint (10 pts)

Before jumping into tool integration, your manager would like you to research what existing analysis tools are out there that can be used with NodeBB. You will evaluate the tools, and eventually document your findings in a design document for your final deliverable.

First, identify and experiment with **at least N-1 potential static and dynamic analysis tools** that are applicable to your system, where N is the number of people in your team. We provide a [starter list of tools](#starter-list-of-tools) in the resources section below to help you get started, but you are not limited to these tools.

In your selection of tools, you should

- have **at least one** static analysis tool
- have **at least one** dynamic analysis tool
- have **at least one** tool that is not from our starter list
- **not use** any of the existing tools within NodeBB as part of your analysis (mocha/ESLint/TSLint)

For each tool that you assess

1. Create a separate testing branch in your repository (named appropriately for the tool you’re testing) to integrate the tool into your project and test out its capabilities
2. Create a pull request to the main branch from each of these testing branches. The PR should have
   
      - **Concrete evidence that you had successfully installed the tool** through trackable file changes demonstrating that extra files/NPM packages were installed.
      - **Artifacts that demonstrate that you have successfully run the tool on your repository.** Acceptable artifacts include output files generated by the tool, or a text file containing the terminal output from the tool; you may also attach screenshots as additional pieces of evidence. They can be attached to the Pull Request in either the description or follow-up comments.

!!! note "Grading Note"
    We will not be grading the quality or quantity of any code you put into these testing branches/PRs, just the evidence that you have successfully installed and run the tool.
    
In your evaluations, consider & experiment with the types of customization that are appropriate or necessary for this tool, both a priori (before they can be used in your project) and over time. Assess the strengths and weaknesses of each tool/technique, both quantitatively and qualitatively.

Consider some of the following questions: 

- What types of problems are you hoping your tooling will catch?  What types of problems does this particular tool catch? 
- What types of customization are possible or necessary? 
- How can/should this tool be integrated into a development process?
- Are there many false positives? False negatives? True positive reports about things you don’t care about?

!!! info "Tool Evaluation"
    There are a lot of different factors to consider when evaluating a tool. We recommend discussing with your teammates and deciding on a group of metrics to focus on when performing evaluations.

!!! note "Time Management"
    Don’t spend too long for this checkpoint. Set deadlines within your team to ensure that you have enough time for both the design document and integration deliverables described below for the final deadline.

By the checkpoint deadline, your team will submit 

- your **initial list of the N-1 tools** that your team plans on exploring, and 
- links to the **PRs that demonstrate that you have successfully installed and run each of these tools**

## Final Deliverables

### Tool Analysis Design Doc (50 pts)

Create a Design Document/RFC that includes:

- A **tools evaluations section** detailing your team’s analysis on each of the tools you experimented with
- A **justification section** explaining which tool(s) you think the project should use moving forward
- An **integration section** describing how the selected tool(s) shall be integrated into your process
- A **conclusion section** summarizing your work

Below, we provide more detailed instructions and page limit recommendations for each of the sections.

#### Tool Evaluations (~N pages)

For each of the N-1 tools explored by your team, you must provide:

- Name and high-level description of what the tool does and a link to its documentation/source
- Whether the tool is used for static or dynamic analysis
- A link to the pull request made from the testing branch for this tool
- A pro/con analysis of the tool and appropriate evidence in the form of screenshots to support your claims. You can use the questions provided in the research you did for the checkpoint to shape your analysis.

Each of these sections should take up approximately 1 page (including screenshots) and no more than 2 pages.

#### Justification (~Half a page)

After going through each of the tools, you should explicitly state the tool(s) you are choosing to integrate for this project and provide a justification for why you are selecting this tool. You should refer to the pro/con analysis done in the prior sections and how they align with the goals of your team and the project overall.

You **must recommend at least one tool**, even if it’s with reservations.

#### Integration (<1 page)

This section should address the different factors to take into consideration when integrating a new tool. At minimum, you should address the following:

- **Technical Questions**
    - How are you integrating the tool (high-level)? At what point in the development/deployment process shall it be integrated? What sorts of customization or configuration will you be using?
    - If you added any specific configuration to allow the main branch of your repository to pass its status checks, add the justification for those decisions in this section.
- **Social Integration Questions** 
    - How do you foresee the team using the tool during their development process? Consider the incentives & deterrents to the developers when it comes to using the tool, and their personal motivation to use it.

Your answers should be based on your experiences running the tools on your team repository and be grounded in data from your research on different factors such as tool usability, output, and customizability.

Keep this section updated as you work on implementing the integration.

#### Conclusion (<1 page)

In this section, provide a brief summary of your findings along with items that were not addressed in the previous sections.

- Are there any open questions?
- Are there any issues you consider to be out of scope?
- What drawbacks of the proposed process/tooling are you accepting for some (good) reason? 

This section should be used to wrap everything up and ensure you have a good/complete design document!

Submit the Design Document as a single PDF to Gradescope.

### Tool Integration (15 pts)

Once you have a tool selected along with a general integration plan, you should fully integrate one of the tools into your project’s workflow. For your checkpoint research, you should have successfully run this tool locally; you should then create a new workflow within the project to run it as part of the development cycle.

Your team should discuss:

- How often should this new integration be run (on each pull request? on commits to main?)
- What level of customization is needed for this tool?
- How should the integration of this tool be enforced?

This configuration must have been justified in your design document under the Integration section.

To be considered successfully integrated, the tool must:

- Be **merged into your main branch**
- **Have been run at least once in the Git flow cycle** (i.e. either during the pull request, merge, or commit stage). 
- **Pass** when run on your codebase. This is indicated by having a green checkmark.

!!! note "Ensuring Passing Checks"
    In order to ensure your checks pass successfully, you may have to make additional changes to your repository, such as fixing reported issues or tweaking tool configuration.
    
    These changes should be documented and addressed in your design document. Continually failing builds show you have *not* completely integrated the tool into your workflow.

On Gradescope, submit a link to your repository and a link to one of the successful GitHub Action runs.

## Extra Credit

Now that you and your classmates have deployed your applications, you will be able to test out each other’s features and provide constructive feedback on your experience and how to improve them! Take this as an opportunity to learn about what your classmates have been working on for the past few weeks. 

Note that this is an **individual** task, unlike the rest of project.

### Feature Review (6 pts)

For extra credit, you will conduct reviews of features developed by three other teams' project. Pick **three** teams's deployment from the [public spreadsheet](https://docs.google.com/spreadsheets/d/1OjXgULfHYlSKW_4aPmBGwMOAJhz_Zww5iFe1rvwSlpQ/edit?usp=sharing) to review, 1 from your own section, and 2 from other sections. 

For each team, you will submit a review of their feature(s). You will need to test the feature(s) as described in their UserGuide and provide feedback on the following:

1. How was the experience of using the feature(s), would this be something you think would help enable better communication between faculty and students and why?
2. How do you think the feature can be improved? and/or What do you think the feature did well in?
3. Did you discover any bugs using the feature(s)?

To qualify for extra credit, you will have to submit your review:

- on Gradescope
- in the appropriate sheet in the [public spreadsheet](https://docs.google.com/spreadsheets/d/1OjXgULfHYlSKW_4aPmBGwMOAJhz_Zww5iFe1rvwSlpQ/edit?usp=sharing). There should be one sheet per team, and you should add your review to the sheet for the team you are reviewing.

## Grading
To receive full credit for the checkpoint, we expect:

- [ ] A link to your successfully deployed web application for your team repository
- [ ] A list of N-1 different static and dynamic analysis tools, where N is the number of members on your team. This list must satisfy all the following criteria:
    - Contain at least one static analysis tool
    - Contain at least one dynamic analysis tool
    - Contain at least one tool not on our starter list of tools
- [ ] Links to N-1 pull requests for each of the selected tools containing evidence of the tool being run at least once on your repository

To receive full credit for the final deadline, we expect:

- [ ] A link to your successfully run CD GitHub action that deploys the website while following proper GitHub practices in handling deployment secrets
- [ ] A design document describing your research into each of the potential tools, justification for your selection of integrated tool(s), and your final integration plan
- [ ] A link to a succuessful run of a GitHub Action that demonstrates your integration of your selected tool(s) into your team workflow

To receive full credit for the extra credit, we expect:

- [ ] Your review of three different teams' features on Gradescope and on the [public spreadsheet](https://docs.google.com/spreadsheets/d/1OjXgULfHYlSKW_4aPmBGwMOAJhz_Zww5iFe1rvwSlpQ/edit?usp=sharing), addressing the three questions described.
## Resources & Documentation

### Starter List of Tools

NodeBB is built in Javascript/Typescript using Node.js and uses Benchpress for its frontend templating. Below are non-exhaustive lists of analysis tools that are available. 

For other resources, [Awesome Static Analysis page](https://github.com/david-a-wheeler/awesome-static-analysis) and [Awesome Dynamic Analysis page](https://github.com/analysis-tools-dev/dynamic-analysis) have extensive listings of available static and dynamic analysis tools for a pretty hefty list of programming languages.

Some of the tools already have existing GitHub Actions workflows on GitHub Marketplace; use your Googling skills, and see what you find!

#### Static Tools

- [flow](https://flow.org/): Static type checker for JavaScript
- [JScent](https://github.com/moskirathe/JScent): Program analyzer for detecting “code smells”
- [JSHint](https://jshint.com/docs/): Used to flag suspicious usage in JavaScript programs
- [StandardJS](https://standardjs.com/)/[ts-standard](https://github.com/standard/ts-standard): Static analysis tool for code quality within JavaScript/TypeScript projects
- [Retire.js](https://retirejs.github.io/retire.js/): Finds library/node module vulnerabilities within your project

#### Dynamic Tools

- [Iroh](https://maierfelix.github.io/Iroh/): Runtime code tracking and visualization
- [Jalangi](https://github.com/Samsung/jalangi2): Framework for dynamic analyses in JavaScript
- [Fast-Fuzz](https://www.npmjs.com/package/fast-fuzz): Fuzzing framework for TypeScript
- [Stryker Mutator](https://stryker-mutator.io/): Mutation testing tool for JavaScript
